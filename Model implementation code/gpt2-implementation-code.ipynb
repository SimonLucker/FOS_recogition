{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport torch.nn.functional as F\nimport csv\n\n#Import fine tuned model\nmodel_GPT2 = torch.load('ROUTE TO MODEL')\n#Import tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to generate a list of variation on figure of speech\ndef generate_figure(\n    model,\n    tokenizer,\n    prompt,\n    entry_count=1,\n    entry_length=15, #maximum number of words\n    top_p=0.8,\n    temperature=0.5,\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n\n    filter_value = -float(\"Inf\")\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n\n            entry_finished = False\n            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                outputs = model(generated, labels=generated)\n                loss, logits = outputs[:2]\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n\n                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n                generated = torch.cat((generated, next_token), dim=1)\n\n                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n                    entry_finished = True\n\n                if entry_finished:\n\n                    generated_num = generated_num + 1\n\n                    output_list = list(generated.squeeze().numpy())\n                    output_text = tokenizer.decode(output_list)\n                    generated_list.append(output_text)\n                    break\n            \n            if not entry_finished:\n              output_list = list(generated.squeeze().numpy())\n              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n              generated_list.append(output_text)\n                \n    return generated_list\n\n#Function to generate multiple sentences.\ndef text_generation(test_data, model):\n  generated_figure = []\n  for i in range(10):\n    x = generate_figure(model.to('cpu'), tokenizer, test_data, entry_count=1)\n    generated_figure.append(x)\n  return generated_figure","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_figure = text_generation(\"INPUT PROMPT HERE\", model_GPT2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to clean the generated figures\ndef clean_generation(generated_figure)\n    counter = 1\n    itemlist = []\n    endproduct_list = []\n    for terms in generated_figure:\n        end_of_sentence = False\n        termsfiltered = ''\n        for term in terms:\n            for char in term:\n                if end_of_sentence == True:\n                    break\n                if((ord(char) >= 97 and ord(char) <= 122) or (ord(char) >= 65 and ord(char) <= 90)) or char in [' ', '.', ',', ':', '?', '!']:\n                    termsfiltered += char\n                if len(termsfiltered) >= 70 and char == \".\":\n                    end_of_sentence = True\n        endproduct = termsfiltered.split('endoftext')\n        print(counter, \"=\" , endproduct[0])\n        print()\n        endproduct_list.append(endproduct[0])\n        counter += 1\n    return endproduct_list\n        ","metadata":{},"execution_count":null,"outputs":[]}]}