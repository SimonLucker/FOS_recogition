{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Install missing library\npip install -U sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom scipy import spatial\n\n#Import the sentence transformer model\nmodel_sentence = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n#Function to embed database\ndef create_embed(input_route, output_route)\n    database = pd.read_csv(input_route)\n    embed_figure_list = []\n    embed_meaning_list = []\n\n    for index, row in tqdm(database.iterrows()):\n        embed_figure = model_sentence.encode(str(row[0]),show_progress_bar=False)\n        embed_meaning = model_sentence.encode(str(row[1]),show_progress_bar=False)\n        embed_figure_list.append(embed_figure)\n        embed_meaning_list.append(embed_meaning)\n\n    df_embeddings = pd.DataFrame({'embed_figures' : []})\n    df_embeddings['embed_figures'] = embed_figure_list\n    df_embeddings['embed_meaning'] = embed_meaning_list\n    df_embeddings.to_csv(output_route, index= True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Get closest rankings to a given input\ndef get_ranking(sentence, database):\n    original_figure_list = []\n    embed_figure_list = []\n    embed_meaning_list = []\n    prediction = predict(model,sentence)\n    print(prediction)\n    \n    if prediction[0] == \"neutral\":\n        return \"Cannot predict for neutral sentence.\"\n    \n    database = database[database['type']==prediction[0]]\n    \n    for index, row in database.iterrows():\n        embed_figure = model_sentence.encode(row[0])\n        embed_meaning = model_sentence.encode(row[1])\n        embed_figure_list.append(embed_figure)\n        embed_meaning_list.append(embed_meaning)\n        original_figure_list.append(row)\n    print(\"row one\",row[0])\n    print(\"row two\",row[1])\n    print(\"row three\",row[2])\n    \n    rankings_figures = []\n    rankings_meaning = []\n    meaningfull_counts = []\n    counter_figures = 0\n    for listing in embed_figure_list:\n        result = 1 - spatial.distance.cosine(sentence, listing)\n        if result > 0.7:\n            rankings_figures.append(listing)\n            meaningfull_counts.append(counter_figures)\n        counter_figures += 1\n        \n    counter_figures = 0\n    for listing in embed_meaning_list:\n        result = 1 - spatial.distance.cosine(sentence, listing)\n        print(result)\n        if result > 0.7:\n            rankings_meaning.append(listing)\n            meaningfull_counts.append(counter_figures)\n        counter_figures += 1\n    \n    if len(meaningfull_counts) == 0:\n        return \"No similar figures of speeches found.\"\n    else:\n        return rankings_meaning, rankings_figures\n        \n        ","metadata":{},"execution_count":null,"outputs":[]}]}