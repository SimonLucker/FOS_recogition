{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport torch.nn.functional as F\nimport csv\n\n#Prepare data\nfigures = pd.read_csv('ROUTE TO DATABASE')\ndf = figures \n\n#Create a very small test set to compare generated text with the reality\ntest_set = df.sample(n = 200)\ndf = df.loc[~df.index.isin(test_set.index)]\n\n#Reset the indexes\ntest_set = test_set.reset_index()\ndf = df.reset_index()\n\n#For the test set only, keep last 20 words in a new column, then remove them from original column\ntest_set['true_type'] = test_set['type'].str.split().str[-20:].apply(' '.join)\ntest_set['type'] = test_set['type'].str.split().str[:-20].apply(' '.join)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prepare class for generated metaphors\nclass metaphor_generation(Dataset):  \n    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.figure = []\n\n        for row in df['type']:\n          self.figure.append(torch.tensor(\n                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n            ))               \n        if truncate:\n            self.figure = self.figure[:20000]\n        self.figure_count = len(self.figure)\n        \n    def __len__(self):\n        return self.figure_count\n\n    def __getitem__(self, item):\n        return self.figure[item]\n    \ndataset = metaphor_generation(df['type'], truncate=True, gpt2_type=\"gpt2\") ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set pretrained tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n#Accumulated batch size (since GPT2 is so big)\ndef pack_tensor(new_tensor, packed_tensor, max_seq_len):\n    if packed_tensor is None:\n        return new_tensor, True, None\n    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n        return packed_tensor, False, new_tensor\n    else:\n        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n        return packed_tensor, True, None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set training parameters\ndef train_gpt(\n    dataset, model, tokenizer,\n    batch_size=50, epochs=10, lr=2e-5,\n    max_seq_len=400, warmup_steps=200,\n    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n    test_mode=False,save_model_on_epoch=False,\n):\n    acc_steps = 100\n    device=torch.device(\"cuda\")\n    model = model.cuda()\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n    )\n\n    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    loss=0\n    accumulating_batch_count = 0\n    input_tensor = None\n\n    for epoch in range(epochs):\n\n        print(f\"Training epoch {epoch}\")\n        print(loss)\n        for idx, entry in tqdm(enumerate(train_dataloader)):\n            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n\n            if carry_on and idx != len(train_dataloader) - 1:\n                continue\n\n            input_tensor = input_tensor.to(device)\n            outputs = model(input_tensor, labels=input_tensor)\n            loss = outputs[0]\n            loss.backward()\n\n            if (accumulating_batch_count % batch_size) == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                model.zero_grad()\n\n            accumulating_batch_count += 1\n            input_tensor = None\n        if save_model_on_epoch:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n            )\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train and save model\nmodel_gpt = train_gpt(dataset, model, tokenizer)\ntorch.save(model, 'ROUTE TO OUTPUT')","metadata":{},"execution_count":null,"outputs":[]}]}