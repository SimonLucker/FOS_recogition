{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom transformers import BertModel, BertConfig\nfrom tqdm import tqdm\n\n\n# Import database\nfos_dataframe = pd.read_csv('ROUTE TO DATABASE')\n\n#Import tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased') \n\n#Create and organise the different labels\nlabels = {'metaphor':0,\n          'metonymy':1,\n          'simile':2,\n          'neutural':3,\n          }\nid2label = {\n    0:'metaphor',\n    1:'metonymy',\n    2:'simile',\n    3:'neutral'\n}\n\n# Create lables for texts\nclass Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, df):\n\n        self.labels = [labels[label] for label in df['type']]\n        self.texts = [tokenizer(str(text),\n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for text in df['word']]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def get_batch_labels(self, idx):\n        # Fetch a batch of labels\n        return np.array(self.labels[idx])\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        batch_y = self.get_batch_labels(idx)\n\n        return batch_texts, batch_y\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create classes for different text types\nclass PredictDataset(torch.utils.data.Dataset):\n    def __init__(self, list_of_strings):\n        self.texts = [tokenizer(str(st),\n                               padding='max_length', max_length = 512, truncation=True,\n                                return_tensors=\"pt\") for st in list_of_strings]\n\n    def __len__(self):\n        return len(self.texts)\n\n    def get_batch_texts(self, idx):\n        # Fetch a batch of inputs\n        return self.texts[idx]\n\n    def __getitem__(self, idx):\n\n        batch_texts = self.get_batch_texts(idx)\n        return batch_texts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test the length of different dataframes for training\nnp.random.seed(112)\ndf_train, df_val, df_test = np.split(fos_dataframe.sample(frac=1, random_state=42), \n                                     [int(.8*len(fos_dataframe)), int(.9*len(fos_dataframe))])\n\nprint(len(df_train),len(df_val), len(df_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set boundries and configurations for the neural network\nconfig = BertConfig\nclass BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n\n        super(BertClassifier, self).__init__()\n\n        self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 5)\n        self.relu = nn.ReLU()\n        self.config = config()\n\n    def forward(self, input_id, mask):\n\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.relu(linear_output)\n\n        return final_layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set training parameters\nEPOCHS = 5\nmodel = BertClassifier()\nLR = 1e-6\n\n#Configure training epochs\ndef train(model, train_data, val_data, learning_rate, epochs):\n\n    train, val = Dataset(train_data), Dataset(val_data)\n\n    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr= learning_rate)\n\n    if use_cuda:\n\n            model = model.cuda()\n            criterion = criterion.cuda()\n\n    for epoch_num in range(epochs):\n\n            total_acc_train = 0\n            total_loss_train = 0\n\n            for train_input, train_label in tqdm(train_dataloader):\n\n                train_label = train_label.to(device)\n                mask = train_input['attention_mask'].to(device)\n                input_id = train_input['input_ids'].squeeze(1).to(device)\n\n                output = model(input_id, mask)\n                \n                batch_loss = criterion(output, train_label.long())\n                total_loss_train += batch_loss.item()\n                \n                acc = (output.argmax(dim=1) == train_label).sum().item()\n                total_acc_train += acc\n\n                model.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            \n            total_acc_val = 0\n            total_loss_val = 0\n\n            with torch.no_grad():\n\n                for val_input, val_label in val_dataloader:\n\n                    val_label = val_label.to(device)\n                    mask = val_input['attention_mask'].to(device)\n                    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n                    output = model(input_id, mask)\n\n                    batch_loss = criterion(output, val_label.long())\n                    total_loss_val += batch_loss.item()\n                    \n                    acc = (output.argmax(dim=1) == val_label).sum().item()\n                    total_acc_val += acc\n            \n            print(\n                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n                | Val Accuracy: {total_acc_val / len(val_data): .3f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train and save the trained model\ntrain(model, df_train, df_val, LR, EPOCHS) \ntorch.save(model, 'ROUTE TO SAVE MODEL')","metadata":{},"execution_count":null,"outputs":[]}]}